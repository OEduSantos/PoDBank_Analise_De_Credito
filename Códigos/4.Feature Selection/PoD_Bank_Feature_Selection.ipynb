{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Definindo a semente\n",
        "random.seed(13)"
      ],
      "metadata": {
        "id": "8ksXGZW93i3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas            as pd\n",
        "import seaborn           as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, auc\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "xVMQI-Qjk7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para gerar metadados\n",
        "def pod_academy_generate_metadata(dataframe):\n",
        "    metadata = pd.DataFrame({\n",
        "        'nome_variavel': dataframe.columns,\n",
        "        'tipo': dataframe.dtypes,\n",
        "        'qt_nulos': dataframe.isnull().sum(),\n",
        "        'percent_nulos': round((dataframe.isnull().sum() / len(dataframe)) * 100, 2),\n",
        "        'cardinalidade': dataframe.nunique(),\n",
        "    })\n",
        "    metadata = metadata.sort_values(by='percent_nulos', ascending=False)\n",
        "    metadata = metadata.reset_index(drop=True)\n",
        "    return metadata\n",
        "\n",
        "# Função para filtrar colunas com alto percentual de nulos\n",
        "def filter_high_null_columns(df, missing_cutoff):\n",
        "    metadata_df = pod_academy_generate_metadata(df)\n",
        "    df_drop_nulos = metadata_df[metadata_df['percent_nulos'] >= missing_cutoff]\n",
        "    lista_drop_nulos = list(df_drop_nulos.nome_variavel.values)\n",
        "    df_filtered = df.drop(columns=lista_drop_nulos)\n",
        "    return df_filtered\n",
        "\n",
        "# Função para converter colunas específicas para int\n",
        "def convert_columns_to_int(df, columns_to_convert):\n",
        "    for col in columns_to_convert:\n",
        "        df[col] = df[col].astype(int)\n",
        "    return df\n",
        "\n",
        "# Função para remover features altamente correlacionadas\n",
        "def remove_highly_correlated_features(df, id_column, target_column, threshold):\n",
        "    object_columns = df.select_dtypes(include=['object']).columns\n",
        "    df_object = df[object_columns]\n",
        "    df_id_target = df[[id_column, target_column]]\n",
        "    df_numeric = df.drop(columns=object_columns.tolist() + [id_column, target_column])\n",
        "    corr_matrix = df_numeric.corr().abs()\n",
        "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
        "    df_reduced = df_numeric.drop(columns=to_drop)\n",
        "    df_reduced = pd.concat([df_id_target, df_reduced, df_object], axis=1)\n",
        "    return df_reduced, to_drop\n",
        "\n",
        "# Função para tratar missings e aplicar encoding\n",
        "def handle_missing_and_encode(X):\n",
        "    imputer_num = SimpleImputer(strategy='mean')\n",
        "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "    cat_attributes = X.select_dtypes(include='object')\n",
        "    num_attributes = X.select_dtypes(exclude='object')\n",
        "    num_imputed = imputer_num.fit_transform(num_attributes)\n",
        "    cat_imputed = imputer_cat.fit_transform(cat_attributes)\n",
        "    df_num = pd.DataFrame(num_imputed, columns=num_attributes.columns)\n",
        "    df_cat = pd.DataFrame(cat_imputed, columns=cat_attributes.columns)\n",
        "    label_encoder = LabelEncoder()\n",
        "    for obj in cat_attributes.columns:\n",
        "        df_cat[obj] = label_encoder.fit_transform(df_cat[obj].astype(str))\n",
        "    return pd.concat([df_num, df_cat], axis=1)\n",
        "\n",
        "# Função para calcular a importância das features\n",
        "def compute_feature_importance(X, y):\n",
        "    algoritmo = GradientBoostingClassifier(random_state=0)\n",
        "    algoritmo.fit(X, y)\n",
        "    feature_importances = algoritmo.feature_importances_\n",
        "    features = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "    return features\n",
        "\n",
        "# Função para selecionar features baseadas na importância\n",
        "def select_important_features(features, cutoff_maximp):\n",
        "    cutoff = cutoff_maximp * features['Importance'].max()\n",
        "    selected_features = features[features['Importance'] > cutoff]\n",
        "    return selected_features\n",
        "\n",
        "# Função principal para selecionar variáveis\n",
        "def vars_selection(df, missing_cutoff, corr_threshold, cutoff_maximp, sample_size=100000):\n",
        "    amostra = df.sample(n=sample_size, random_state=13)\n",
        "    amostra = filter_high_null_columns(amostra, missing_cutoff)\n",
        "    columns_to_convert = ['CNT_CHILDREN', 'TARGET', 'FLAG_MOBIL', 'FLAG_WORK_PHONE',\n",
        "                          'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL'] + \\\n",
        "                         [col for col in amostra.columns if col.startswith('FLAG_DOCUMENT_')]\n",
        "    amostra = convert_columns_to_int(amostra, columns_to_convert)\n",
        "    amostra, dropped_corr = remove_highly_correlated_features(amostra, 'SK_ID_CURR', 'TARGET', corr_threshold)\n",
        "    X = amostra.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
        "    y = amostra['TARGET']\n",
        "    X = handle_missing_and_encode(X)\n",
        "    features = compute_feature_importance(X, y)\n",
        "    selected_features = select_important_features(features, cutoff_maximp)\n",
        "    return selected_features\n",
        "\n",
        "# Função para plotar a importância das features\n",
        "def plot_feature_importance(selected_features):\n",
        "    selected_features = selected_features.sort_values(by='Importance', ascending=True)\n",
        "    plt.figure(figsize=(10, len(selected_features)*0.4))\n",
        "    plt.barh(selected_features['Feature'], selected_features['Importance'], color=(0.25, 0.5, 1))\n",
        "    plt.xlabel(\"Feature Importance\")\n",
        "    plt.title(\"Variáveis Selecionadas - Gradient Boosting\")\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2_Uvv56a0wdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_treino_full = pd.read_parquet('PoD Bank/Tabelas - Feature_Engineering/ABT/train.parquet')\n",
        "df_teste_full = pd.read_parquet('PoD Bank/Tabelas - Feature_Engineering/ABT/test.parquet')\n",
        "df_validacao_full = pd.read_parquet('PoD Bank/Tabelas - Feature_Engineering/ABT/validation.parquet')"
      ],
      "metadata": {
        "id": "aTnBL6b0lG4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selected_features_df = vars_selection(df_treino_full, 75, 0.85, 0)\n",
        "df_train = df_treino_full[list(selected_features_df.Feature) + ['TARGET']]\n",
        "df_test = df_teste_full[list(selected_features_df.Feature)+ ['TARGET']]\n",
        "df_valid = df_validacao_full[list(selected_features_df.Feature)]"
      ],
      "metadata": {
        "id": "o_iSORsd2Fzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape, df_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27tPJ_zw_3SC",
        "outputId": "70bea42f-7fb2-41fe-a9b6-3c867adb4a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150580, 130), (64677, 130), (92254, 129))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_parquet('abt_train_fselect.parquet')\n",
        "df_test.to_parquet('abt_test_fselect.parquet')\n",
        "df_valid.to_parquet('abt_valid_fselect.parquet')"
      ],
      "metadata": {
        "id": "Fk8Ps8b4LXfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPBSi3I2L0ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}