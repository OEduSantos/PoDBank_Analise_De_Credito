{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqR3UgrYEU4k"
      },
      "source": [
        "## Configuração do ambiente para utilização do Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw4e2Q1kCIDQ"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Fazendo download\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Descompactando os arquivos\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Importando a biblioteca os\n",
        "import os\n",
        "\n",
        "# Definindo a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Definindo a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "# instalando a findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Importando a findspark\n",
        "import findspark\n",
        "\n",
        "# Iniciando o findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Feature Engineering\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwbtAlBEZAs"
      },
      "source": [
        "## Leitura dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNV6vEW-EKdX"
      },
      "outputs": [],
      "source": [
        "#Leitura do arquivo CSV\n",
        "dados = spark.read.csv(\"PoD Bank/credit_card_balance.csv\",header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTbU7ilrE4Vm"
      },
      "source": [
        "## Criação de flags para nos auxiliar na visão temporal dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pON8yHJ0Ehu7"
      },
      "outputs": [],
      "source": [
        "## Habilitando uso do SparkSQL\n",
        "dados.createOrReplaceTempView(\"dados\")\n",
        "\n",
        "df_temp_01 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN MONTHS_BALANCE >= -3 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_3_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -6 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_6_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -12 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_12_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -18 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_18_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -24 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_24_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -30 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_30_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -36 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_36_meses\n",
        "FROM dados\n",
        "ORDER BY `SK_ID_PREV`;\n",
        "\"\"\")\n",
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGKJkIdFTM3"
      },
      "source": [
        "## Sumarizar na visão cliente (Automatizada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPYsoQsqFM1K"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, round, sum, avg, max, min, when\n",
        "\n",
        "# Definir as colunas para agregação\n",
        "colunas_agregacao_total = df_temp_01.columns\n",
        "colunas_agregacao_total.remove('SK_ID_CURR')\n",
        "colunas_agregacao_total.remove('SK_ID_PREV')\n",
        "colunas_agregacao_total.remove('MONTHS_BALANCE')\n",
        "\n",
        "colunas_flags = ['ultimos_3_meses','ultimos_6_meses', 'ultimos_12_meses', 'ultimos_18_meses',  'ultimos_24_meses',  'ultimos_30_meses',\n",
        "'ultimos_36_meses']\n",
        "\n",
        "expressoes_agregacao = []\n",
        "\n",
        "for flag in colunas_flags:\n",
        "  for coluna in colunas_agregacao_total:\n",
        "    if 'DPD' in coluna:\n",
        "      expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MAX_{coluna.upper()}_{flag.upper()}\"))\n",
        "      expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MIN_{coluna.upper()}_{flag.upper()}\"))\n",
        "    else:\n",
        "      expressoes_agregacao.append(round(sum(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_TOT_{coluna.upper()}_{flag.upper()}\"))\n",
        "      expressoes_agregacao.append(round(avg(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MED_{coluna.upper()}_{flag.upper()}\"))\n",
        "      expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MAX_{coluna.upper()}_{flag.upper()}\"))\n",
        "      expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MIN_{coluna.upper()}_{flag.upper()}\"))\n",
        "\n",
        "expressoes_agregacao = tuple(expressoes_agregacao)\n",
        "\n",
        "# Aplicar as expressões de agregação\n",
        "df_temp_02 = df_temp_01.groupBy(\"SK_ID_PREV\").agg(*expressoes_agregacao).orderBy(\"SK_ID_PREV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-0L-0WSG4W8"
      },
      "source": [
        "## Salvar a tabela sumarizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cVJG3BG5IO"
      },
      "outputs": [],
      "source": [
        "df_temp_02 = df_temp_02.repartition(1)\n",
        "df_temp_02.write.mode(\"overwrite\").parquet(\"PoD Bank/Tabelas - Feature_Engineering/credit_card_balance_agg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-hTDg6ZOWjH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}