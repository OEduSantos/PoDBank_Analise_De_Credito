{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração do ambiente para utilização do Spark"
      ],
      "metadata": {
        "id": "uht2ZYnXaN5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2R8VhGtaInW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Fazendo download\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Descompactando os arquivos\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Importando a biblioteca os\n",
        "import os\n",
        "\n",
        "# Definindo a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Definindo a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "# instalando a findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Importando a findspark\n",
        "import findspark\n",
        "\n",
        "# Iniciando o findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Minha Primeira Aplicação no Pyspark\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leitura dos dados"
      ],
      "metadata": {
        "id": "SorcWSzFaY7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo arquivo CSV\n",
        "dados = spark.read.csv(\"/content/drive/MyDrive/Estudos/Ciencia de Dados/PoD Bank/bureau_balance.csv\",header=True)\n"
      ],
      "metadata": {
        "id": "qpjAPlPvaPgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação de flags para nos auxiliar na visão temporal dos dados"
      ],
      "metadata": {
        "id": "uKqaU8fFbDuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Habilitando uso do SparkSQL\n",
        "dados.createOrReplaceTempView(\"dados\")\n",
        "\n",
        "df_temp_01 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN MONTHS_BALANCE >= -3 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_3_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -6 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_6_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -12 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_12_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -18 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_18_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -24 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_24_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -30 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_30_meses,\n",
        "    CASE\n",
        "        WHEN MONTHS_BALANCE >= -36 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_36_meses\n",
        "FROM dados\n",
        "ORDER BY `SK_ID_BUREAU`;\n",
        "\"\"\")\n",
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n"
      ],
      "metadata": {
        "id": "gbmpuZL2a1ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Habilitando uso do SparkSQL\n",
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n",
        "\n",
        "df_temp_02 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN STATUS = \"C\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_C,\n",
        "    CASE\n",
        "        WHEN STATUS = \"3\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_3,\n",
        "    CASE\n",
        "        WHEN STATUS = \"0\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_0,\n",
        "    CASE\n",
        "        WHEN STATUS = \"2\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_2,\n",
        "    CASE\n",
        "        WHEN STATUS = \"X\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_X,\n",
        "    CASE\n",
        "        WHEN STATUS = \"5\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_5,\n",
        "    CASE\n",
        "        WHEN STATUS = \"1\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS STATUS_1\n",
        "FROM df_temp_01\n",
        "ORDER BY `SK_ID_BUREAU`;\n",
        "\"\"\")\n",
        "df_temp_02.createOrReplaceTempView(\"df_temp_01\")"
      ],
      "metadata": {
        "id": "yGbTVSnFbPvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumarizar na visão cliente (Automatizada)"
      ],
      "metadata": {
        "id": "lVvuRPG9bgAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, sum, avg, max, min, when, count\n",
        "\n",
        "# Definir as colunas para agregação\n",
        "colunas_agregacao_total = ['STATUS_C','STATUS_3','STATUS_0','STATUS_2','STATUS_X','STATUS_5','STATUS_1']\n",
        "\n",
        "colunas_flags = ['ultimos_3_meses','ultimos_6_meses', 'ultimos_12_meses', 'ultimos_18_meses',  'ultimos_24_meses',  'ultimos_30_meses',\n",
        "'ultimos_36_meses']\n",
        "expressoes_agregacao = []\n",
        "\n",
        "for flag in colunas_flags:\n",
        "  for coluna in colunas_agregacao_total:\n",
        "    expressoes_agregacao.append(round(count(when(col(flag) == 1, col(coluna))), 2).alias(f\"QTD_{coluna.upper()}_{flag.upper()}\"))\n",
        "\n",
        "expressoes_agregacao = tuple(expressoes_agregacao)\n",
        "\n",
        "# Aplicar as expressões de agregação\n",
        "df_temp_03 = df_temp_02.groupBy(\"SK_ID_BUREAU\").agg(*expressoes_agregacao).orderBy(\"SK_ID_BUREAU\")\n"
      ],
      "metadata": {
        "id": "tnyAE3W8beYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvar a tabela sumarizada"
      ],
      "metadata": {
        "id": "RylATli2cTTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_03 = df_temp_03.repartition(1)\n",
        "df_temp_03.write.mode(\"overwrite\").parquet(\"PoD Bank/Tabelas - Feature_Engineering/bureau_balance_agg.\")"
      ],
      "metadata": {
        "id": "wVcG4vCPbqCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8WSgWhLcYLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}