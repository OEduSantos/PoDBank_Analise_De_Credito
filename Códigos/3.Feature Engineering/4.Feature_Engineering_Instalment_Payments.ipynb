{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração do ambiente para utilização do Spark"
      ],
      "metadata": {
        "id": "Yu4nJQQjNQNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJtWmoVNIMW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Fazendo download\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Descompactando os arquivos\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Importando a biblioteca os\n",
        "import os\n",
        "\n",
        "# Definindo a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Definindo a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "# instalando a findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Importando a findspark\n",
        "import findspark\n",
        "\n",
        "# Iniciando o findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Minha Primeira Aplicação no Pyspark\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leitura dos dados"
      ],
      "metadata": {
        "id": "R5qYic7zNTka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Leitura do arquivo CSV\n",
        "dados = spark.read.csv(\"PoD Bank/installments_payments.csv\",header=True)\n"
      ],
      "metadata": {
        "id": "8DDsxcOgNPoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação de flags para nos auxiliar na visão temporal dos dados"
      ],
      "metadata": {
        "id": "BcwLnrf4N0vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Habilitando uso do SparkSQL\n",
        "dados.createOrReplaceTempView(\"dados\")\n",
        "\n",
        "df_temp_01 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN DAYS_INSTALMENT >= -90 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_3_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -180 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_6_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -360 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_12_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -540 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_18_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -720 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_24_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -900 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_30_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_INSTALMENT >= -1080 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_36_meses\n",
        "FROM dados\n",
        "ORDER BY `SK_ID_PREV`;\n",
        "\"\"\")\n",
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n"
      ],
      "metadata": {
        "id": "lZe9jcwXNh6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumarizar na visão cliente (Automatizada)"
      ],
      "metadata": {
        "id": "LbALIZvHOmBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, sum, avg, max, min, when\n",
        "\n",
        "# Definir as colunas para agregação\n",
        "colunas_agregacao_total = df_temp_01.columns\n",
        "colunas_agregacao_total.remove('SK_ID_CURR')\n",
        "colunas_agregacao_total.remove('SK_ID_PREV')\n",
        "\n",
        "colunas_flags = ['ultimos_3_meses','ultimos_6_meses', 'ultimos_12_meses', 'ultimos_18_meses',  'ultimos_24_meses',  'ultimos_30_meses',\n",
        "'ultimos_36_meses']\n",
        "expressoes_agregacao = []\n",
        "\n",
        "for flag in colunas_flags:\n",
        "  for coluna in colunas_agregacao_total:\n",
        "    if 'DAY' in coluna:\n",
        "      expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MAX_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "      expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MIN_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "    else:\n",
        "      expressoes_agregacao.append(round(sum(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_TOT_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "      expressoes_agregacao.append(round(avg(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MED_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "      expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MAX_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "      expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MIN_{coluna.upper()}_{flag.upper()}_INSTALLMENTS\"))\n",
        "\n",
        "expressoes_agregacao = tuple(expressoes_agregacao)\n",
        "\n",
        "# Aplicar as expressões de agregação\n",
        "df_temp_02 = df_temp_01.groupBy(\"SK_ID_PREV\").agg(*expressoes_agregacao).orderBy(\"SK_ID_PREV\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xcyTiRC_OQpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvar a tabela sumarizada"
      ],
      "metadata": {
        "id": "1HBY3vaTO1OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_02 = df_temp_02.repartition(1)\n",
        "df_temp_02.write.mode(\"overwrite\").parquet(\"PoD Bank/Tabelas - Feature_Engineering/installments_agg\")"
      ],
      "metadata": {
        "id": "oTqDQSZoOwHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o7nQFVMpO4du"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}