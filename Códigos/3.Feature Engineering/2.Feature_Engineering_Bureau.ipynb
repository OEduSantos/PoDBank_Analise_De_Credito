{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração do ambiente para utilização do Spark"
      ],
      "metadata": {
        "id": "BqgcO-8skc4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roSxQOISkX4R"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Fazendo download\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Descompactando os arquivos\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Importando a biblioteca os\n",
        "import os\n",
        "\n",
        "# Definindo a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Definindo a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "# instalando a findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Importando a findspark\n",
        "import findspark\n",
        "\n",
        "# Iniciando o findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Criando a sessão do Spark com configuração de memória aumentada\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Feature Engineering\") \\\n",
        "    .config(\"spark.executor.memory\", \"40g\") \\\n",
        "    .config(\"spark.driver.memory\", \"40g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.cores\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo arquivo csv\n",
        "dados = spark.read.csv(\"PoD Bank/bureau.csv\",header=True)\n"
      ],
      "metadata": {
        "id": "WS1TShSFkfWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Habilitando uso do SparkSQL\n",
        "dados.createOrReplaceTempView(\"dados\")\n",
        "\n",
        "df_temp_01 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN DAYS_CREDIT >= -90 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_3_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -180 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_6_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -360 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_12_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -540 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_18_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -720 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_24_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -900 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_30_meses,\n",
        "    CASE\n",
        "        WHEN DAYS_CREDIT >= -1080 THEN 1\n",
        "        ELSE 0\n",
        "    END AS ultimos_36_meses\n",
        "FROM dados\n",
        "ORDER BY `SK_ID_BUREAU`;\n",
        "\"\"\")\n",
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")"
      ],
      "metadata": {
        "id": "aYAnNRjvk02b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n",
        "\n",
        "df_temp_02 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN CREDIT_ACTIVE = \"Closed\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_ACTIVE_CLOSED,\n",
        "    CASE\n",
        "        WHEN CREDIT_ACTIVE = \"Active\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_ACTIVE_ACTIVE,\n",
        "    CASE\n",
        "        WHEN CREDIT_ACTIVE = \"Sold\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_ACTIVE_SOLD,\n",
        "    CASE\n",
        "        WHEN CREDIT_ACTIVE = \"Bad debt\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_ACTIVE_BAD_DEBT\n",
        "FROM df_temp_01\n",
        "ORDER BY `SK_ID_BUREAU`;\n",
        "\"\"\")\n",
        "df_temp_02.createOrReplaceTempView(\"df_temp_01\")\n"
      ],
      "metadata": {
        "id": "ICEkuhZ0lGco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_02.createOrReplaceTempView(\"df_temp_02\")\n",
        "\n",
        "df_temp_03 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "      CASE\n",
        "        WHEN CREDIT_CURRENCY = \"currency 1\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_CURRENCY_currency_1,\n",
        "    CASE\n",
        "        WHEN CREDIT_CURRENCY = \"currency 2\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_CURRENCY_currency_2,\n",
        "    CASE\n",
        "        WHEN CREDIT_CURRENCY = \"currency 3\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_CURRENCY_currency_3,\n",
        "    CASE\n",
        "        WHEN CREDIT_CURRENCY = \"currency 4\" THEN 1\n",
        "        ELSE 0\n",
        "    END AS CREDIT_CURRENCY_currency_4\n",
        "FROM df_temp_02\n",
        "ORDER BY `SK_ID_BUREAU`;\n",
        "\"\"\")\n",
        "df_temp_03.createOrReplaceTempView(\"df_temp_02\")\n"
      ],
      "metadata": {
        "id": "toHVQuDWllKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumarizar na visão cliente (Automatizada)"
      ],
      "metadata": {
        "id": "ypaT6rqbltzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, sum, avg, max, min, when\n",
        "\n",
        "# Função para gerar expressões de agregação com flags\n",
        "def gerar_expressoes_agregacao_com_flags(colunas_agregacao_total, colunas_flags):\n",
        "    expressoes_agregacao = []\n",
        "\n",
        "    for flag in colunas_flags:\n",
        "        for coluna in colunas_agregacao_total:\n",
        "            if 'DAY' in coluna:\n",
        "                expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MAX_{coluna.upper()}_{flag.upper()}\"))\n",
        "                expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MIN_{coluna.upper()}_{flag.upper()}\"))\n",
        "            else:\n",
        "                expressoes_agregacao.append(round(sum(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_TOT_{coluna.upper()}_{flag.upper()}\"))\n",
        "                expressoes_agregacao.append(round(avg(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MED_{coluna.upper()}_{flag.upper()}\"))\n",
        "                expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MAX_{coluna.upper()}_{flag.upper()}\"))\n",
        "                expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"VL_MIN_{coluna.upper()}_{flag.upper()}\"))\n",
        "\n",
        "    return expressoes_agregacao\n",
        "\n",
        "# Função para gerar expressões de agregação sem flags, apenas com categorias\n",
        "def gerar_expressoes_agregacao_sem_flags(colunas_agregacao_total, colunas_cat):\n",
        "    expressoes_agregacao = []\n",
        "\n",
        "    for cat in colunas_cat:\n",
        "        for coluna in colunas_agregacao_total:\n",
        "            if 'DAY' in coluna:\n",
        "                expressoes_agregacao.append(round(max(when(col(cat) == 1, col(coluna))), 2).alias(f\"QT_MAX_{coluna.upper()}_{cat.upper()}\"))\n",
        "                expressoes_agregacao.append(round(min(when(col(cat) == 1, col(coluna))), 2).alias(f\"QT_MIN_{coluna.upper()}_{cat.upper()}\"))\n",
        "            else:\n",
        "                expressoes_agregacao.append(round(sum(when(col(cat) == 1, col(coluna))), 2).alias(f\"VL_TOT_{coluna.upper()}_{cat.upper()}\"))\n",
        "                expressoes_agregacao.append(round(avg(when(col(cat) == 1, col(coluna))), 2).alias(f\"VL_MED_{coluna.upper()}_{cat.upper()}\"))\n",
        "                expressoes_agregacao.append(round(max(when(col(cat) == 1, col(coluna))), 2).alias(f\"VL_MAX_{coluna.upper()}_{cat.upper()}\"))\n",
        "                expressoes_agregacao.append(round(min(when(col(cat) == 1, col(coluna))), 2).alias(f\"VL_MIN_{coluna.upper()}_{cat.upper()}\"))\n",
        "\n",
        "    return expressoes_agregacao\n",
        "\n",
        "# Definir as colunas para agregação\n",
        "colunas_agregacao_total = ['CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE',\n",
        "                           'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT',\n",
        "                           'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY']\n",
        "\n",
        "colunas_flags = ['ultimos_3_meses','ultimos_6_meses', 'ultimos_12_meses', 'ultimos_18_meses',  'ultimos_24_meses',  'ultimos_30_meses',\n",
        "'ultimos_36_meses']\n",
        "colunas_cat1 = ['CREDIT_ACTIVE_CLOSED', 'CREDIT_ACTIVE_ACTIVE', 'CREDIT_ACTIVE_SOLD', 'CREDIT_ACTIVE_BAD_DEBT']\n",
        "colunas_cat2 = ['CREDIT_CURRENCY_currency_1', 'CREDIT_CURRENCY_currency_2', 'CREDIT_CURRENCY_currency_3', 'CREDIT_CURRENCY_currency_4']\n",
        "\n",
        "# Gerar expressões de agregação para flags\n",
        "expressoes_agregacao_flags = gerar_expressoes_agregacao_com_flags(colunas_agregacao_total, colunas_flags)\n",
        "\n",
        "# Gerar expressões de agregação para categorias (sem flags)\n",
        "expressoes_agregacao_cat1 = gerar_expressoes_agregacao_sem_flags(colunas_agregacao_total, colunas_cat1)\n",
        "expressoes_agregacao_cat2 = gerar_expressoes_agregacao_sem_flags(colunas_agregacao_total, colunas_cat2)\n",
        "\n",
        "# Combinar todas as expressões de agregação\n",
        "expressoes_agregacao = expressoes_agregacao_flags + expressoes_agregacao_cat1 + expressoes_agregacao_cat2\n",
        "\n",
        "# Aplicar as expressões de agregação\n",
        "df_temp_04 = df_temp_03.groupBy(\"SK_ID_BUREAU\").agg(*expressoes_agregacao).orderBy(\"SK_ID_BUREAU\")\n",
        "\n"
      ],
      "metadata": {
        "id": "41v-KAbNlrxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join com a bureau balance"
      ],
      "metadata": {
        "id": "6djAAWzfmQC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bureau_balance = spark.read.parquet(\"PoD Bank/Tabelas - Feature_Engineering/bureau_balance.parquet\")"
      ],
      "metadata": {
        "id": "kPxZ3rnxmM46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_05 = df_temp_04.join(bureau_balance, on='SK_ID_BUREAU', how='left')\n"
      ],
      "metadata": {
        "id": "2gVI4omDmbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join deste resultado a base bureau"
      ],
      "metadata": {
        "id": "qJV-6RjWo7X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_06 = df_temp_03.select(\"SK_ID_BUREAU\", \"SK_ID_CURR\").join(df_temp_05, on='SK_ID_BUREAU', how='left')\n"
      ],
      "metadata": {
        "id": "S5Col-K6meeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumarizar base resultante na visão cliente"
      ],
      "metadata": {
        "id": "AExTI_YYpIEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir as colunas para agregação\n",
        "colunas_agregacao_total = df_temp_06.columns\n",
        "colunas_agregacao_total.remove('SK_ID_CURR')\n",
        "colunas_agregacao_total.remove('SK_ID_BUREAU')\n",
        "\n",
        "expressoes_agregacao = []\n",
        "\n",
        "for coluna in colunas_agregacao_total:\n",
        "  if 'DAY' in coluna:\n",
        "    expressoes_agregacao.append(round(max(col(coluna)), 2).alias(f\"QT_MAX_{coluna.upper()}\"))\n",
        "    expressoes_agregacao.append(round(min(col(coluna)), 2).alias(f\"QT_MIN_{coluna.upper()}\"))\n",
        "  else:\n",
        "    expressoes_agregacao.append(round(sum(col(coluna)), 2).alias(f\"VL_TOT_{coluna.upper()}\"))\n",
        "    expressoes_agregacao.append(round(avg(col(coluna)), 2).alias(f\"VL_MED_{coluna.upper()}\"))\n",
        "    expressoes_agregacao.append(round(max(col(coluna)), 2).alias(f\"VL_MAX_{coluna.upper()}\"))\n",
        "    expressoes_agregacao.append(round(min(col(coluna)), 2).alias(f\"VL_MIN_{coluna.upper()}\"))\n",
        "\n",
        "expressoes_agregacao = tuple(expressoes_agregacao)\n",
        "\n",
        "# Aplicar as expressões de agregação\n",
        "df_temp_07 = df_temp_06.groupBy(\"SK_ID_CURR\").agg(*expressoes_agregacao).orderBy(\"SK_ID_CURR\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vBJkyZ9ipD9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_07 = df_temp_07.repartition(1)\n",
        "df_temp_07.write.mode(\"overwrite\").parquet(\"PoD Bank/Tabelas - Feature_Engineering/bureau_agg\")"
      ],
      "metadata": {
        "id": "WgwAh5SBpRP2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}